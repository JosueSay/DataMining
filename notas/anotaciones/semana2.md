# Proceso de Machine Learning (ML) ü§ñ

## Preprocesamiento de Datos üßπ

1. **Obtener los datos**:
   - Este paso no es trivial; se requiere ser un investigador con acceso a fuentes adecuadas. Por ejemplo, datos de Guatemala disponibles en la Universidad de Texas.
2. **Limpiar los datos**:
   - Eliminar errores, datos duplicados, o valores inconsistentes.
3. **Divisi√≥n entrenamiento/prueba**:
   - Separar los datos en dos conjuntos: uno para entrenar los modelos y otro para probarlos.
   - Problema com√∫n: el sobreentrenamiento (overfitting), donde el modelo funciona bien con los datos de entrenamiento, pero falla con datos nuevos.
4. **Escalamiento de caracter√≠sticas**:
   - Normalizar o estandarizar las variables para evitar que valores peque√±os o grandes influyan desproporcionadamente en el modelo.

## Modelaci√≥n üõ†Ô∏è

1. **Construir el modelo**:
   - Utilizar modelos existentes y ajustar los par√°metros seg√∫n los datos espec√≠ficos.
2. **Entrenar el modelo**:
   - Alimentar los datos de entrenamiento al modelo.
3. **Hacer predicciones**:
   - Aplicar el modelo entrenado para predecir resultados con datos nuevos.

## Evaluaci√≥n üìä

1. **Calcular m√©tricas del rendimiento**:
   - Utilizar m√©tricas como exactitud, precisi√≥n, o recall seg√∫n el caso.
2. **Emitir un veredicto**:
   - Determinar si el modelo cumple con los requisitos esperados.

## Conjunto de Entrenamiento y Prueba üß™

La validaci√≥n final puede fallar por varias razones, como:

- Problemas en la toma de datos.
- Errores durante la limpieza.
- Insuficiente cantidad de datos usados para el entrenamiento.

![Conjunto de Entrenamiento y Prueba](../images/entrenamiento_prueba.png "Conjunto de Entrenamiento y Prueba")

## Escalamiento de Caracter√≠sticas üìè

El escalamiento por columna es crucial para evitar volatilidad en el modelo. Valores extremos (muy grandes o peque√±os) pueden distorsionar los resultados.

![Estandarizaci√≥n](../images/estandarizacion.png "Estandarizaci√≥n")

### Ejemplo

Se comparan salarios y edades entre tres individuos representados por colores:

| Color   | Salario  | Edad |
|---------|----------|------|
| Azul    | 70,000   | 45   |
| Morado  | 60,000   | 44   |
| Rojo    | 52,000   | 40   |

**Estandarizaci√≥n de los datos:**

| Color   | Salario | Edad |
|---------|---------|------|
| Azul    | 1       | 1    |
| Morado  | 0.444   | 0.75 |
| Rojo    | 0       | 0    |

Tras la estandarizaci√≥n, se observa que **el m√°s parecido al Morado es el Azul**, ya que:

- En salario, ambos est√°n casi iguales.
- En edad, el Azul est√° m√°s cerca del Morado que el Rojo.

## Clustering üß©

### Conceptos B√°sicos

- **Observaciones**: Registros, es decir, las mediciones.
- **Caracter√≠sticas**: Columnas o variables.

El clustering, desde un enfoque estad√≠stico, agrupa observaciones basadas en las variables que las describen, es decir, sus caracter√≠sticas.

A diferencia de la clasificaci√≥n o regresi√≥n (donde el modelo se entrena con datos conocidos), el clustering pertenece al **aprendizaje no supervisado**, ya que el modelo identifica los grupos sin intervenci√≥n directa.

### Ejemplo de Clusters

Si agrupamos pa√≠ses como Canad√°, Estados Unidos, Francia, Reino Unido, Alemania y Australia en tres clusters:

- **C1**: Canad√°, Estados Unidos.
- **C2**: Alemania, Reino Unido, Francia.
- **C3**: Australia.

Este agrupamiento podr√≠a basarse en continentes.

En otro caso, agrup√°ndolos en dos clusters:

- **C1**: Canad√°, Estados Unidos, Reino Unido, Australia.
- **C2**: Alemania, Francia.

Aqu√≠ podr√≠amos interpretarlo como pa√≠ses de habla inglesa y no inglesa.

Cuando usamos una sola variable, el clustering es sencillo, pero al incorporar m√∫ltiples variables (multivariables), podemos obtener **insights** m√°s profundos.

### Objetivo del Clustering

- **Maximizar la similitud** entre los elementos dentro de un cluster.
- **Minimizar la disimilitud** entre los clusters.

---

## Diferencias entre Clustering y Clasificaci√≥n üìä

- **Clustering**: Aprendizaje no supervisado. Los datos proporcionan salidas que nosotros interpretamos, sin conocer previamente el n√∫mero exacto de grupos ni su utilidad.
- **Clasificaci√≥n**: Aprendizaje supervisado. Se entrena el modelo con ejemplos etiquetados.

### Aplicaciones del Clustering

- **Segmentaci√≥n de mercados**:
  - Identificar clientes para nuevos productos.
  - Determinar ubicaciones ideales para tiendas.
- **Segmentaci√≥n de im√°genes**.
- An√°lisis en diversas √°reas como biolog√≠a, geograf√≠a, y m√°s.

---

## An√°lisis de Grupos (Clusters) üîç

Pasos principales:

1. Identificar **problemas de agrupamiento**.
2. Realizar an√°lisis de grupos.
3. Determinar el **n√∫mero √≥ptimo de grupos**.
4. Identificar las **caracter√≠sticas apropiadas** (eliminar columnas innecesarias que confundan el modelo).
5. **Interpretar los resultados**.

### K-Means Clustering

El m√©todo m√°s conocido es **K-Means Clustering**:

- **K**: N√∫mero de grupos deseados.
- **Means**: Media utilizada para encontrar los centroides.

#### Algoritmo K-Means

1. Especificar el valor de **K**.
2. Seleccionar semillas iniciales (centroides) al azar.
3. Repetir:
   - Asignar cada punto al centroide m√°s cercano.
   - Calcular los nuevos centroides.
4. Finalizar cuando las posiciones de los centroides no cambien.

**Nota**: Se utilizan distancias (por ejemplo, la f√≥rmula de Pit√°goras) para determinar las cercan√≠as en un espacio con m√∫ltiples dimensiones (caracter√≠sticas).

---

## M√©todo del Codo para Determinar el N√∫mero de Clusters üí°

Se utiliza el **WCSS (Within-Cluster Sum of Squares)**:

1. Graficar WCSS contra el n√∫mero de clusters.
2. Identificar visualmente el punto donde la reducci√≥n del WCSS comienza a ser marginal ("el codo").

Esto ayuda a encontrar una cantidad √≥ptima de clusters para que el modelo sea eficiente sin sobredimensionar.

---

## Ventajas y Desventajas de K-Means

### Ventajas ‚úÖ

- F√°cil de comprender e implementar.
- R√°pido en agrupar.
- Ampliamente disponible.
- Siempre produce un resultado.

### Desventajas ‚ùå

1. **Elegir el valor de K**:
   - Puede ser subjetivo; el m√©todo del codo ayuda a mitigarlo.
2. **Sensibilidad a la inicializaci√≥n**:
   - La posici√≥n inicial de los centroides afecta los resultados. Soluci√≥n: usar K-Means++.
3. **Sensibilidad a valores at√≠picos**:
   - Soluci√≥n: eliminar valores at√≠picos, ya que el c√°lculo se basa en la media y no en la mediana.
4. **Resultados esf√©ricos**:
   - Estandarizar los datos puede ser necesario para evitar clusters distorsionados.

---

## Estandarizaci√≥n y Normalizaci√≥n üåê

Estandarizar balancea el peso de las caracter√≠sticas para evitar que valores grandes (e.g., precios) dominen al modelo.

Ejemplo:

- **Precio**: Millones.
- **√Årea**: Miles.

Sin estandarizar, los valores grandes de precio pueden causar distorsiones.

Sin embargo, en ciertos casos, no estandarizar es deseable si algunas caracter√≠sticas tienen mayor peso o relevancia (e.g., precio en terrenos).

### Sesgo de Variables Omitidas (OVB)

Omitir variables importantes, intencionalmente o no, puede generar resultados sesgados. Incorporar m√°s caracter√≠sticas o aplicar clustering puede ayudar a detectar estos problemas y mejorar la interpretaci√≥n de los datos.
